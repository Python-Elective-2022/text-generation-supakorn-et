{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4Bf5FVHfganK",
        "outputId": "1c3c0912-ca97-42ad-b871-49c36aa4bdfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-26 05:32:57--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.125.139, 142.250.125.101, 142.250.125.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.125.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/89a6gc4s7enkhk3tc61g2k6iu8mv5ljf/1679808750000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=13ebb333-072c-4055-9d30-c452ec7b2497 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-03-26 05:33:00--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/89a6gc4s7enkhk3tc61g2k6iu8mv5ljf/1679808750000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=13ebb333-072c-4055-9d30-c452ec7b2497\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 173.194.198.132, 2607:f8b0:4001:c1c::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|173.194.198.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M  80.8MB/s    in 0.9s    \n",
            "\n",
            "2023-03-26 05:33:02 (80.8 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "apcEXp7WhVBs",
        "outputId": "9836bc07-76b2-4af5-a6de-8322c8921e0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-fbdddccf8583>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n"
          ]
        }
      ],
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zsmu3aEId49i",
        "outputId": "5e2639b9-7632-45a9-f48e-bc07c3ab3043",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "G1YXuxIqfygN",
        "outputId": "ad107215-d570-4406-e204-80e1ebe9dd2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 16s 85ms/step - loss: 5.9912 - accuracy: 0.0303\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 1s 17ms/step - loss: 5.4350 - accuracy: 0.0399\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.3656 - accuracy: 0.0368\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.3099 - accuracy: 0.0394\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.2423 - accuracy: 0.0409\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 5.1820 - accuracy: 0.0399\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 5.1285 - accuracy: 0.0394\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 5.0765 - accuracy: 0.0404\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 5.0246 - accuracy: 0.0424\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 4.9510 - accuracy: 0.0540\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 1s 16ms/step - loss: 4.8716 - accuracy: 0.0560\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.7976 - accuracy: 0.0610\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 4.7118 - accuracy: 0.0762\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.6317 - accuracy: 0.0777\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.5519 - accuracy: 0.0888\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.4705 - accuracy: 0.0984\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.3867 - accuracy: 0.1054\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.3204 - accuracy: 0.1140\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.2366 - accuracy: 0.1297\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.1669 - accuracy: 0.1438\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.1014 - accuracy: 0.1493\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 4.0294 - accuracy: 0.1539\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 3.9551 - accuracy: 0.1680\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.8995 - accuracy: 0.1806\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.8222 - accuracy: 0.1922\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 3.7479 - accuracy: 0.2205\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.6979 - accuracy: 0.2311\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.6198 - accuracy: 0.2457\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.5563 - accuracy: 0.2578\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.4891 - accuracy: 0.2740\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.4316 - accuracy: 0.2871\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 3.3806 - accuracy: 0.2911\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 3.3157 - accuracy: 0.3068\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.2580 - accuracy: 0.3169\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.2027 - accuracy: 0.3285\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.1415 - accuracy: 0.3421\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.0916 - accuracy: 0.3507\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.0291 - accuracy: 0.3602\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9738 - accuracy: 0.3850\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9150 - accuracy: 0.4031\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.8653 - accuracy: 0.4092\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.8279 - accuracy: 0.4107\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.7532 - accuracy: 0.4334\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.7034 - accuracy: 0.4324\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6523 - accuracy: 0.4521\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6152 - accuracy: 0.4627\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5659 - accuracy: 0.4642\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5180 - accuracy: 0.4778\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4896 - accuracy: 0.4803\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4453 - accuracy: 0.4945\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3883 - accuracy: 0.4990\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3341 - accuracy: 0.5126\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3279 - accuracy: 0.5166\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2756 - accuracy: 0.5202\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2351 - accuracy: 0.5293\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.1865 - accuracy: 0.5338\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.1383 - accuracy: 0.5474\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.0976 - accuracy: 0.5580\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.0506 - accuracy: 0.5621\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.0117 - accuracy: 0.5727\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9748 - accuracy: 0.5848\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9339 - accuracy: 0.5913\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9034 - accuracy: 0.5969\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8690 - accuracy: 0.6060\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8537 - accuracy: 0.6039\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8308 - accuracy: 0.6049\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7930 - accuracy: 0.6160\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7492 - accuracy: 0.6327\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7504 - accuracy: 0.6387\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7305 - accuracy: 0.6362\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6822 - accuracy: 0.6483\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6413 - accuracy: 0.6599\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6080 - accuracy: 0.6650\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5810 - accuracy: 0.6746\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5533 - accuracy: 0.6771\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5359 - accuracy: 0.6816\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5523 - accuracy: 0.6756\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5368 - accuracy: 0.6751\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4781 - accuracy: 0.6902\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4441 - accuracy: 0.6983\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4183 - accuracy: 0.7053\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3856 - accuracy: 0.7134\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 1.3736 - accuracy: 0.7159\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.3633 - accuracy: 0.7210\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.3428 - accuracy: 0.7235\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.3165 - accuracy: 0.7351\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2874 - accuracy: 0.7447\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2697 - accuracy: 0.7412\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2520 - accuracy: 0.7487\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2305 - accuracy: 0.7487\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2837 - accuracy: 0.7240\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2530 - accuracy: 0.7407\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2276 - accuracy: 0.7422\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.1910 - accuracy: 0.7513\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1509 - accuracy: 0.7624\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1344 - accuracy: 0.7689\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1482 - accuracy: 0.7659\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1234 - accuracy: 0.7689\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0961 - accuracy: 0.7679\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0841 - accuracy: 0.7735\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0639 - accuracy: 0.7765\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0507 - accuracy: 0.7805\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0346 - accuracy: 0.7846\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0237 - accuracy: 0.7851\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0146 - accuracy: 0.7861\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0197 - accuracy: 0.7825\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0350 - accuracy: 0.7699\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9952 - accuracy: 0.7856\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.9615 - accuracy: 0.8002\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.9521 - accuracy: 0.7987\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.9364 - accuracy: 0.8007\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9269 - accuracy: 0.8027\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9097 - accuracy: 0.8073\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9015 - accuracy: 0.8108\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8917 - accuracy: 0.8123\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.8765 - accuracy: 0.8118\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8614 - accuracy: 0.8133\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8552 - accuracy: 0.8118\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8372 - accuracy: 0.8214\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8279 - accuracy: 0.8249\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8245 - accuracy: 0.8194\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8113 - accuracy: 0.8254\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7932 - accuracy: 0.8259\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7853 - accuracy: 0.8315\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.8004 - accuracy: 0.8254\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8022 - accuracy: 0.8290\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7827 - accuracy: 0.8320\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8149 - accuracy: 0.8194\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8001 - accuracy: 0.8290\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7761 - accuracy: 0.8264\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7517 - accuracy: 0.8375\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7349 - accuracy: 0.8391\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7275 - accuracy: 0.8401\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7261 - accuracy: 0.8426\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 0.7109 - accuracy: 0.8406\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.6960 - accuracy: 0.8446\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.8461\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6819 - accuracy: 0.8476\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6702 - accuracy: 0.8522\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6606 - accuracy: 0.8542\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6542 - accuracy: 0.8552\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6480 - accuracy: 0.8562\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6457 - accuracy: 0.8547\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6501 - accuracy: 0.8522\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6316 - accuracy: 0.8507\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6225 - accuracy: 0.8582\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6157 - accuracy: 0.8613\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6353 - accuracy: 0.8552\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6617 - accuracy: 0.8365\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6269 - accuracy: 0.8577\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6127 - accuracy: 0.8582\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6291 - accuracy: 0.8522\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5959 - accuracy: 0.8602\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5847 - accuracy: 0.8618\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5803 - accuracy: 0.8648\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5799 - accuracy: 0.8638\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5756 - accuracy: 0.8628\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5641 - accuracy: 0.8693\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5605 - accuracy: 0.8683\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5552 - accuracy: 0.8718\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5431 - accuracy: 0.8744\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.5362 - accuracy: 0.8794\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5353 - accuracy: 0.8749\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5281 - accuracy: 0.8749\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5220 - accuracy: 0.8729\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5274 - accuracy: 0.8739\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5187 - accuracy: 0.8744\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5241 - accuracy: 0.8724\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5256 - accuracy: 0.8724\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5335 - accuracy: 0.8668\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5134 - accuracy: 0.8754\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5004 - accuracy: 0.8789\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5451 - accuracy: 0.8713\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5463 - accuracy: 0.8643\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4999 - accuracy: 0.8789\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4969 - accuracy: 0.8809\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4780 - accuracy: 0.8804\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4692 - accuracy: 0.8870\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4692 - accuracy: 0.8865\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4641 - accuracy: 0.8840\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4549 - accuracy: 0.8885\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4511 - accuracy: 0.8860\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4463 - accuracy: 0.8890\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.4419 - accuracy: 0.8905\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.4385 - accuracy: 0.8890\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4349 - accuracy: 0.8890\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4317 - accuracy: 0.8905\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4338 - accuracy: 0.8900\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4733 - accuracy: 0.8749\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4568 - accuracy: 0.8829\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4421 - accuracy: 0.8870\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4345 - accuracy: 0.8865\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4393 - accuracy: 0.8814\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4296 - accuracy: 0.8835\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4186 - accuracy: 0.8900\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4158 - accuracy: 0.8915\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4088 - accuracy: 0.8915\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4098 - accuracy: 0.8910\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4070 - accuracy: 0.8895\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4034 - accuracy: 0.8946\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aeSNfS7uhch0",
        "outputId": "2046126d-94f6-4036-a0aa-26d1fe2f596f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DC7zfcgviDTp",
        "outputId": "1b9c0534-461e-4dba-b8c9-c5046a886f1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 701ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "im feeling chills me sound you did before on leaving together quiet about on on on hes so so sky crying pavement handle may front would blue song oh song oh final night park house pavement mad do feeling warm return to not front door final scars morning power power be end but on let but on on end do strong were end had strong strong the scars theyre leaving end door door end on be end on on fate park morning do final scars theyre leaving leaving end here end here here get on our was above talk end on talk so\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l10c03_nlp_constructing_text_generation_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}